Name: 'Transformer-DeID: Deidentification of free-text clinical notes with transformers'
Description: Sharing patient data such as clinical notes among clinicians and researchers
  is fundamental to advancements in clinical practice and biomedical research. Prior
  to distribution, it is often necessary and legally mandated to remove identifiers
  such as names, contact details, identification numbers, and dates to safeguard patient
  privacy. This process is known as deidentification. Here we provide a neural network
  model for the removal of patient identifiers from clinical text based upon Bidirectional
  Encoder Representations from Transformers (BERT) and two variations of BERT, namely
  Robustly Optimized BERT Approach (RoBERTa) and Distilled BERT (DistilBERT). The
  models demonstrated excellent performance on a publicly available benchmark and
  are made freely available to maximize reuse.
Documentation: https://doi.org/10.13026/7dj5-7x85
Contact: https://physionet.org/about/#contact_us
UpdateFrequency: Not updated
Tags:
- aws-pds
License: MIT License
Resources:
- Description: https://doi.org/10.13026/7dj5-7x85
  ARN: arn:aws:s3:::physionet-open/transformer-deid/
  Region: us-east-1
  Type: S3 Bucket
